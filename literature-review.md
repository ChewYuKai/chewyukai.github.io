# Literature Review

## Papers

| Name | Date | Category | Summary |
| :--- | :--- | :--- | :--- |
| Deep Residual Learning for Image Recognition | 21/07/2019 | Architecture | Residual Networks make use of "shortcuts" connections which allow succeeding layers to build upon features learnt from previous layers. Shortcut connections also have the added benefit of allowing gradients to flow through them and hence, side-stepping the vanishing gradient problem. |
| Deep Networks with Stochastic Depth | 21/07/2019 | Architecture | Residual Networks with Stochastic Depth takes the idea of 'dropout' to residual blocks. Having the constant c in, x' = x + cf\(x\) being set to zero at random, with linearly-increasing probability as each deeper layers during training. The benefit is two-fold: Firstly, dropping out layers at time speed-up training of deep networks, and secondly, it allows layers that varying depth to communicate with each other. |
| Densely Connected Convolutional Networks | 28/07/2017 | Architecture |  |
| Neural Ordinary Differential Equation | 28/07/2017 | Architecture |  |
| An Overview of Multi-Task Learning in Deep Neural Networks | 28/07/2017 | Architecture |  |
| Solving Multiclass Learning Problems via Error-Correcting Output Codes | 28/07/2017 | Misc |  |
| Rich feature hierarchies for accurate object detection and semantic segmentation |  |  | R-CNN |
| Fast R-CNN |  |  | Fast R-CNN |
| Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks |  |  | Faster R-CNN |
| Beyond one-hot encoding: Lower dimensional target embedding |  |  |  |
| Self-Supervised GAN via Auxillary Rotation Loss |  |  |  |
| Few-Shot Adversarial Learning of Realistic Neural Talking Head Model |  |  |  |
| Noise2Noise: Learning Image Restoration without Clean Data |  |  |  |

