# Literature Review

## Conference Papers

| Name | Date | Category | Summary |
| :--- | :--- | :--- | :--- |
| Deep Residual Learning for Image Recognition | 21/07/2019 | Architecture | Residual Networks make use of "shortcuts" connections which allow succeeding layers to build upon features learnt from previous layers. Shortcut connections also have the added benefit of allowing gradients to flow through them and hence, side-stepping the vanishing gradient problem. |
| Deep Networks with Stochastic Depth | 21/07/2019 | Architecture | Residual Networks with Stochastic Depth takes the idea of 'dropout' to residual blocks. Having the constant c in, x' = x + cf\(x\) being set to zero at random, with linearly-increasing probability as each deeper layers during training. The benefit is two-fold: Firstly, dropping out layers at time speed-up training of deep networks, and secondly, it allows layers that varying depth to communicate with each other. |
| Densely Connected Convolutional Networks | 28/07/2017 | Architecture |  |

